[{"authors":[],"categories":[],"content":"Welcome.\n","date":"2024年3月22日","img":"/news/2024/03/welcome/feature.png","lang":"zh-hans","langName":"简体中文","largeImg":"/news/2024/03/welcome/feature_hu4b4547b9c4fda6194c6ec6c997b8c6ab_17474359_500x0_resize_box_3.png","permalink":"/news/2024/03/welcome/","series":[{"title":"News","url":"/series/news/"}],"smallImg":"/news/2024/03/welcome/feature_hu4b4547b9c4fda6194c6ec6c997b8c6ab_17474359_180x0_resize_box_3.png","tags":[],"timestamp":1711115691,"title":"Welcome"},{"authors":[],"categories":[],"content":" 数据的导入 数据的导入需要用到两个 PyTorch 库，分别是 torch.utils.data.Dataset 和 torch.utils.data.DataLoader\nDataset 类的使用 可以使用 Dataset 类处理自定义的数据集，示例如下：\nclass MyDataset(Dataset): def __init__(self, root_dir, label_dir): self.root_dir = root_dir self.label_dir = label_dir self.path = os.path.join(self.root_dir, self.label_dir) self.image_name = os.listdir(self.path) def __getitem__(self, idx): img_name = self.image_name[idx] img_path = os.path.join(self.path, img_name) label = self.label_dir img = Image.open(img_path) return img, label def __len__(self): return len(self.image_name) 自定义自己的数据类需要继承 PyTorch 的 Dataset 类。\n方法的重写 继承 Dataset 父类后，还需要根据数据集的具体结构和任务需求重写一些方法，其中必须重写的方法包括：__init__ 、__getitem__、__len__\n构造方法的重写 重写构造方法的目的是定义其他方法所需的属性，例如上文示例中的根路径、标签路径等等。\ngetitem 方法的重写 除了对象 self 以外，__getitem__ 方法接受一个额外的变量 idx，表示数据集中一个数据的索引。\n我们需要重写该方法，根据传入的索引返回将要输入模型的特征和标签。\nlen 方法的重写 重写该方法以定义数据集的长度，例如上文示例用图片数量定义数据集长度\n使用已有数据集 这些提供的数据集使用方式很简单，只需要按照 PyTorch 文档中的使用方法填写参数，只需一行代码即可。\n例如 CIFAR10 的使用方法如下：\ntest_set = torchvision.datasets.CIFAR10( \u0026#34;./dataset\u0026#34;, train=False, transform=torchvision.transforms.ToTensor(), download=True ) 数据的处理 处理图片 PyTorch 在 torchvision.transforms 中提供了许多处理图片的工具，例如常用的工具类 torchvision.transforms.ToTensor() 用于将 PIL 或 ndarray (使用 opencv) 格式的图片转换为 PyTorch 的 Tensor 类型，以便输入模型。\n除此之外，transform 模块还提供了常见的图片处理工具，例如图片反转、切割等等。\nDataLoader 类的使用 test_loader = DataLoader(dataset=test_set, batch_size=64, shuffle=True, num_workers=0, drop_last=False) 遍历数据集的方法：\nfor data in test_loader: imgs, targets = data 搭建模型 搭建模型有两种方式，分别是：\n从零搭建 从已有的模型搭建 从零搭建模型 首先，定义一个继承 nn.module 的模型类，然后重写方法即可，示例如下：\nclass Model(nn.Module): def __init__(self): super(Model, self).__init__() self.model = nn.Sequential( nn.Conv2d(3, 32, 5, 1, 2), nn.MaxPool2d(2), nn.LeakyReLU(), nn.Conv2d(32, 32, 5, 1, 2), nn.MaxPool2d(2), nn.LeakyReLU(), nn.Conv2d(32, 64, 5, 1, 2), nn.MaxPool2d(2), nn.LeakyReLU(), nn.Flatten(), nn.Linear(64 * 4 * 4, 256), nn.ReLU(), nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 10), ) def forward(self, x): x = self.model(x) return x 从已有的模型搭建模型 导入已有模型 获取已有的模型有两种方式，第一种方式是使用 PyTorch 提供的知名模型，例如 vgg16。\nvgg16_pt = torchvision.models.vgg16(pretrained=True) 参数 pretrained 为 True 时会下载预训练后的模型参数，反之则使用未训练的随机模型参数\n修改已有模型 有时候，下载的模型无法满足我们的任务需求，例如处理分类问题时，模型的输出层的神经元个数与我们需要区分的类别数量不同。这个时候，我们就需要修改模型。\n修改模型所用到的函数或方法主要有以下几个：\n# 1. 增加：add_module 方法，可以增加单层也可以增加 Sequential vgg16.add_module(\u0026#34;mod\u0026#34;, nn.Sequential(OrderedDict([ # 可以不给每层取名 (\u0026#34;linear\u0026#34;, nn.Linear(1000, 256)), (\u0026#34;relu\u0026#34;, nn.ReLU()), (\u0026#34;softmax\u0026#34;, nn.Softmax(10)) ]))) vgg16.add_module(nn.Linear(64, 10)) # 2. 修改：直接使用 [idx] 和 `.` print(vgg16) # 先print查看模型结构 vgg16.classifier[6] = nn.Linear(64, 10) vgg16.classifier[6] = nn.Sequential( nn.Linear(64, 10), nn.Linear(10, 5) ) # 3. 删除：使用 del 即可 del vgg16.classifier 训练模型 下面提供一个训练示例：\n# * 3. Create the model object and Setting hyperparameters # create model object model = Model() if torch.cuda.is_available(): model = model.cuda() # define the loss function loss_fn = nn.CrossEntropyLoss() if torch.cuda.is_available(): loss_fn = loss_fn.cuda() # define the optimizer learning_rate = 0.01 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # optimizer = torch.optim.Adam(model.parameters()) # Setting total_train_step = 0 total_test_step = 0 epoch = 30 # set tensorboard writer = SummaryWriter(\u0026#34;./logs_model\u0026#34;) # * 4. Training the model for i in range(epoch): print(f\u0026#34;------ Epoch {i} begin. ------\u0026#34;) start_time = time.time() # train model.train() # for some certain layer(such as dropout), See PyTorch.org. for data in train_dataloader: imgs, targets = data # move to gpu if torch.cuda.is_available(): imgs, targets = imgs.cuda(), targets.cuda() output = model(imgs) loss = loss_fn(output, targets) # optimize optimizer.zero_grad() loss.backward() optimizer.step() total_train_step = total_train_step + 1 if total_train_step % 100 == 0: print(f\u0026#34;Train step: {total_train_step} / Loss: {loss}\u0026#34;) end_time = time.time() print(f\u0026#34;tic {end_time-start_time}\u0026#34;) writer.add_scalar(\u0026#34;train_loss(step)\u0026#34;, loss, total_train_step) # test model.eval() # for some certain layer total_test_loss = 0 total_accuracy = 0 with torch.no_grad(): for data in test_dataloader: imgs, targets = data # move to gpu if torch.cuda.is_available(): imgs, targets = imgs.cuda(), targets.cuda() output = model(imgs) # compute accuracy accuracy = (output.argmax(1) == targets).sum() total_accuracy += accuracy loss = loss_fn(output, targets) total_test_loss += loss # Show and log test info print(ColorText.info(f\u0026#34;\\nTotal loss in test set: {total_test_loss}\u0026#34;)) print( ColorText.info(f\u0026#34;Total accuracy in test set: {total_accuracy / test_set_size}\u0026#34;) ) writer.add_scalar(\u0026#34;test_loss(epoch)\u0026#34;, total_test_loss, total_test_step) writer.add_scalar( \u0026#34;test_accuracy(epoch)\u0026#34;, total_accuracy / test_set_size, total_test_step ) total_test_step += 1 # Save Model torch.save(model, f\u0026#34;model_{i}.pt\u0026#34;) print(ColorText.info(f\u0026#34;..::Model {i} Saved..\u0026#34;)) writer.close() 附录 Tensorboard 的使用 示例：\n首先在脚本内记录数据到指定路径\nfrom torch.urils.tensorboard import SummaryWriter # writer 对象 writer = SummaryWriter(\u0026#34;./logs\u0026#34;) # log_dir # 记录数据 writer.add_scalar(\u0026#34;loss curve\u0026#34;, y_value, x_value) # writer.close() 然后，命令行用 Tensorboard 打开日志路径\ntensorboard --log_dir ./logs ","date":"2024年3月22日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/blog/2024/03/pytorch/","series":[],"smallImg":"","tags":[{"title":"深度学习","url":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"title":"PyTorch","url":"/tags/pytorch/"}],"timestamp":1711112701,"title":"PyTorch"},{"authors":[],"categories":[],"content":"W.I.P 动态规划学习笔记.\n动态规划的演进 Dfs -\u0026gt; 记忆化搜索 -\u0026gt; 动态规划 (倒序递推/逆序递推 -\u0026gt; 空间优化)\n实现记忆化搜索，dfs 函数的参数应当尽量少，不影响边界的参数不要添加。 想要剪枝，一般多用参数\n动态规划 什么是动态规划 动态规划（英语：Dynamic programming，简称 DP），是一种在数学、管理科学、计算机科学、经济学和生物信息学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。 动态规划常常适用于有重叠子问题和最优子结构性质的问题。\n动态规划的核心思想： 动态规划最核心的思想，就在于拆分子问题，记住过往，减少重复计算。\n示例： 下题\n// P1359 租用游艇 // https://www.luogu.com.cn/problem/P1359 #include \u0026lt;bits/stdc++.h\u0026gt; #define int long long using namespace std; const int N = 1000200; int timee[210][210]; // 直接从 i 到 j 的时间 int n; int mintime[210][210]; // 子问题分解，记录从 i 到 j 的最短时间 signed main(void) { cin \u0026gt;\u0026gt; n; for (int i=1; i\u0026lt;=n; i++) { for (int j=i+1; j\u0026lt;=n; j++) { cin \u0026gt;\u0026gt; timee[i][j]; } } // input part for (int i=1; i\u0026lt;=n; i++) { // 递推，从 1 到 i 的最短时间 -\u0026gt; 从 1 到 n 的最短时间 for (int j=i; j\u0026gt;=1; j--) { // 最后一步 if (j == i) mintime[1][i] = timee[1][i]; //！注意 else mintime[1][i] = min(mintime[1][i], timee[j][i] + mintime[1][j]); } } cout \u0026lt;\u0026lt; mintime[1][n]; return 0; } ","date":"2024年3月22日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/posts/dynamic/","series":[],"smallImg":"","tags":[{"title":"算法，动态规划","url":"/tags/%E7%AE%97%E6%B3%95%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"timestamp":1711112641,"title":"动态规划"},{"authors":[],"categories":[],"content":"[[搜索]]\n深度优先搜索（DFS） [[深度优先搜索]] 深度优先搜索的步骤分为 1.递归下去 2.回溯上来。 顾名思义，深度优先，则是以深度为准则，先一条路走到底，直到达到目标。这里称之为递归下去。否则既没有达到目标又无路可走了，那么则退回到上一步的状态，走其他路。这便是回溯上来。\n[! EXAMPLE] 把正整数 $n$ 分解为 $3$ 个不同的正整数，如 $6=1+2+3$，排在后面的数必须大于等于前面的数，输出所有方案\n原理：\nint m, arr[103]; // 决策需要参数 n：剩下的数 i:决策层数 a:上层决策结果 void dfs(int n, int i, int a) { if (n==0) { //输出，边界条件 for (int j = 0; j \u0026lt;= i; j++) printf(\u0026#34;%d \u0026#34;, arr[j]); printf(\u0026#34;\\n\u0026#34;); } if (i\u0026lt;=m) { for (int j = a; j \u0026lt;= n; j++) { arr[i] = j; // 存下当前决策 dfs(n - j, i + 1, j); // 下一层决策 } } } 代码：Luogu P1706 全排列问题\n#include \u0026lt;iomanip\u0026gt; #include \u0026lt;iostream\u0026gt; using namespace std; int n; bool vis[50]; // 访问标记数组 int a[50]; // 排列数组，按顺序储存当前搜索结果 void dfs(int step) { if (step == n + 1) { // 边界 for (int i = 1; i \u0026lt;= n; i++) { cout \u0026lt;\u0026lt; setw(5) \u0026lt;\u0026lt; a[i]; // 保留5个场宽 } cout \u0026lt;\u0026lt; endl; return; } for (int i = 1; i \u0026lt;= n; i++) { if (vis[i] == 0) { // 判断数字i是否在正在进行的全排列中 vis[i] = 1; a[step] = i; dfs(step + 1); vis[i] = 0; // 这一步不使用该数 置0后允许下一步使用 } } return; } int main() { cin \u0026gt;\u0026gt; n; dfs(1); return 0; } 关于状态回溯等 TIPS [!状态回溯]\n在进行每一层决策时，要遍历当前层所有的情况。我们通常选定当前的决策，然后根据这一层的决策进行下一层决策（也就是开启下一层 dps）。然后先清除之前的决策，再接着遍历当前层的其他\n例子 1：两种选择 -\u0026gt; 直接依次决策 kkksc03考前临时抱佛脚\nvoid dps(int i, int s_type) { if (i \u0026gt; s[s_type]) { min_sub = min ( min_sub, max(left0, right0) ); return; }\tleft0 += time0[s_type][i]; //先决策左脑 dps( i + 1, s_type); //依据当前层决策左脑的决策， 继续 dps left0 -= time0[s_type][i]; //清楚当前层决策 right0 += time0[s_type][i]; //决策右脑 dps( i + 1, s_type); right0 -= time0[s_type][i]; } 例子 2：n 种选择 -\u0026gt; 循环决策 [USACO1.5] 八皇后 Checker Challenge\nvoid queen(int i) { if (i \u0026gt;= n+1) { print(); return; } else { for (int k=1; k\u0026lt;=n; k++) // 循环 { if (b[k]==0 \u0026amp;\u0026amp; c[i+k]==0 \u0026amp;\u0026amp; d[i-k+n]==0) { a[i] = k; b[k] = 1; c[i+k] = 1; d[i-k + n] = 1; queen(i + 1); // dps b[k] = 0; // 清除 c[i+k] = 0; d[i-k + n] = 0; } } } } 例题 自然数的拆分问题\nint a[10001]={1},n; void search(int tot, int i) { if (tot == 0 \u0026amp;\u0026amp; i != 2) { //不能只输出一个数 int k; for (k=1; k\u0026lt;=i - 2; k++) { cout\u0026lt;\u0026lt;a[k]\u0026lt;\u0026lt;\u0026#34;+\u0026#34;; } cout\u0026lt;\u0026lt;a[k]\u0026lt;\u0026lt;endl; // i-1 return; } else { for (int j=a[i-1]; j\u0026lt;=tot; j++) //大于等于前一个数 { if (i\u0026lt;=n) // 拆分数不能比n大 { a[i] = j; search(tot - j, i + 1); // a[i] = 1; } } } } int main() { cin\u0026gt;\u0026gt;n; search(n,1); return 0; } 广度优先搜索（BFS） ","date":"2024年3月22日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/blog/2024/03/%E6%90%9C%E7%B4%A2/","series":[],"smallImg":"","tags":[],"timestamp":1711112641,"title":"搜索"},{"authors":[],"categories":[],"content":"Hi there, I\u0026rsquo;m XXX.\n","date":"2019年2月28日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/about/","series":[],"smallImg":"","tags":[],"timestamp":1551312000,"title":"关于我"},{"authors":[],"categories":[],"content":"","date":"1年1月1日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/offline/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Offline"},{"authors":[],"categories":[],"content":"","date":"1年1月1日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/contact/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"联系我们"}]
