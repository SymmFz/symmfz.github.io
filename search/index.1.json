[{"authors":[],"categories":[{"title":"Blog","url":"/categories/blog/"}],"content":"Zotero 是一款简洁、强大的文献管理工具，为研究者提供了高效便捷的文献管理方案。本文将带你快速入门 Zotero，介绍 Zotero 的安装与使用方法，分享必备插件，带你初步构建高效科研文献管理系统。\n前言 一个高效的文献管理系统能够极大的提高论文阅读体验和科研效率。然而，想要实现高效有序的文献管理，仅仅依靠人力来完成是不现实的，你可能需要：\n手动下载论文 手动打上大量的标签 手动组织文件体系 显然，运用软件辅助实现文献系统管理才是终极方案，而 Zotero 也许正是你的最优选择。\nZotero 是一款简洁、强大的文献管理工具，为研究者提供了高效便捷的文献管理方案。接下来本文将带你快速入门 Zotero，介绍 Zotero 的安装与使用方法，分享必备插件，带你初步构建高效科研文献管理系统。\n什么是 Zotero Zotero 是一款开源、高效的文献管理工具，它具有以下优点：\n高度自动化： Zotero 可以自动从论文网站、书籍、期刊文章和其他在线资源中提取作者、年份等关键信息，简化文献信息的录入过程。 功能强大：Zotero 具备众多强大功能，助力用户高效管理文献。 开源免费： Zotero 是开源软件，可以免费使用。 信息安全： Zotero 的数据是本地存储的，并且支持云端同步，为用户的信息安全提供了可靠保障。 插件丰富： Zotero拥有活跃的插件社区，提供丰富多样的插件，为用户提供了更多功能扩展的可能性。 跨平台 Zotero 的安装 下载链接：Zotero Downloads 下载 Zotero 以及浏览器插件（必要） 安装过程无脑 Next 即可，可以修改安装路径。\nZotero 基本用法 创建分类 菜单栏： 文件 -\u0026gt; 新建分类 -\u0026gt; 输入分类名 添加文献 一般使用 Jasminum、Sci-Hub 等插件自动抓取文献，不需要手动添加。\n必备 Zotero 插件 插件安装方法 菜单栏：工具 -\u0026gt; 附加组件 下载插件的 .xpi 文件，拖到 Adds-on Manager 里，点击安装。 Jasminum - 茉莉花 下载 Jasminum - 茉莉花 中国用户必备插件，抓取中文文献，可用于自动下载知网文献。\n安装后需要更新中文翻译器：编辑 -\u0026gt; 首选项 -\u0026gt; 茉莉花 -\u0026gt; 非官方维护中文翻译器 -\u0026gt; 更新全部 中文插件设置 -\u0026gt; 打开知网，输入知网链接后登录知网： 保证 Zotero 在后台运行，打开知网文献，点击 Zotero 插件即可自动抓取文献到 Zotero。 Zotero-scihub 下载 Zotero-scihub 安装 Zotero-scihub 后能够从 Sci-Hub 自动下载外国文献。 其他插件 Zotfile - 自动重命名，移动，并附加 PDF (或其他文件)到 Zotero 的项目 Translate for Zotero - 翻译英文文献 ","date":"2024年3月24日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/blog/2024/03/zotero-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8-%E6%9E%84%E5%BB%BA%E9%AB%98%E6%95%88%E7%A7%91%E7%A0%94%E6%96%87%E7%8C%AE%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/","series":[{"title":"软件教程","url":"/series/%E8%BD%AF%E4%BB%B6%E6%95%99%E7%A8%8B/"}],"smallImg":"","tags":[{"title":"Zotero","url":"/tags/zotero/"},{"title":"快速入门","url":"/tags/%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/"},{"title":"科研","url":"/tags/%E7%A7%91%E7%A0%94/"},{"title":"软件推荐","url":"/tags/%E8%BD%AF%E4%BB%B6%E6%8E%A8%E8%8D%90/"},{"title":"文献管理","url":"/tags/%E6%96%87%E7%8C%AE%E7%AE%A1%E7%90%86/"},{"title":"教程","url":"/tags/%E6%95%99%E7%A8%8B/"}],"timestamp":1711287111,"title":"Zotero 快速入门 - 构建高效科研文献管理系统"},{"authors":[],"categories":[],"content":"本文为 PyTorch 学习笔记，介绍了 PyTorch 项目的基本代码范式。\n数据的导入 数据的导入需要用到两个 PyTorch 库，分别是 torch.utils.data.Dataset 和 torch.utils.data.DataLoader\nDataset 类的使用 可以使用 Dataset 类处理自定义的数据集，示例如下：\nclass MyDataset(Dataset): def __init__(self, root_dir, label_dir): self.root_dir = root_dir self.label_dir = label_dir self.path = os.path.join(self.root_dir, self.label_dir) self.image_name = os.listdir(self.path) def __getitem__(self, idx): img_name = self.image_name[idx] img_path = os.path.join(self.path, img_name) label = self.label_dir img = Image.open(img_path) return img, label def __len__(self): return len(self.image_name) 自定义自己的数据类需要继承 PyTorch 的 Dataset 类。\n方法的重写 继承 Dataset 父类后，还需要根据数据集的具体结构和任务需求重写一些方法，其中必须重写的方法包括：__init__ 、__getitem__、__len__\n构造方法的重写 重写构造方法的目的是定义其他方法所需的属性，例如上文示例中的根路径、标签路径等等。\ngetitem 方法的重写 除了对象 self 以外，__getitem__ 方法接受一个额外的变量 idx，表示数据集中一个数据的索引。\n我们需要重写该方法，根据传入的索引返回将要输入模型的特征和标签。\nlen 方法的重写 重写该方法以定义数据集的长度，例如上文示例用图片数量定义数据集长度\n使用已有数据集 这些提供的数据集使用方式很简单，只需要按照 PyTorch 文档中的使用方法填写参数，只需一行代码即可。\n例如 CIFAR10 的使用方法如下：\ntest_set = torchvision.datasets.CIFAR10( \u0026#34;./dataset\u0026#34;, train=False, transform=torchvision.transforms.ToTensor(), download=True ) 数据的处理 处理图片 PyTorch 在 torchvision.transforms 中提供了许多处理图片的工具，例如常用的工具类 torchvision.transforms.ToTensor() 用于将 PIL 或 ndarray (使用 opencv) 格式的图片转换为 PyTorch 的 Tensor 类型，以便输入模型。\n除此之外，transform 模块还提供了常见的图片处理工具，例如图片反转、切割等等。\nDataLoader 类的使用 test_loader = DataLoader(dataset=test_set, batch_size=64, shuffle=True, num_workers=0, drop_last=False) 遍历数据集的方法：\nfor data in test_loader: imgs, targets = data 搭建模型 搭建模型有两种方式，分别是：\n从零搭建 从已有的模型搭建 从零搭建模型 首先，定义一个继承 nn.module 的模型类，然后重写方法即可，示例如下：\nclass Model(nn.Module): def __init__(self): super(Model, self).__init__() self.model = nn.Sequential( nn.Conv2d(3, 32, 5, 1, 2), nn.MaxPool2d(2), nn.LeakyReLU(), nn.Conv2d(32, 32, 5, 1, 2), nn.MaxPool2d(2), nn.LeakyReLU(), nn.Conv2d(32, 64, 5, 1, 2), nn.MaxPool2d(2), nn.LeakyReLU(), nn.Flatten(), nn.Linear(64 * 4 * 4, 256), nn.ReLU(), nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 10), ) def forward(self, x): x = self.model(x) return x 从已有的模型搭建模型 导入已有模型 获取已有的模型有两种方式，第一种方式是使用 PyTorch 提供的知名模型，例如 vgg16。\nvgg16_pt = torchvision.models.vgg16(pretrained=True) 参数 pretrained 为 True 时会下载预训练后的模型参数，反之则使用未训练的随机模型参数\n修改已有模型 有时候，下载的模型无法满足我们的任务需求，例如处理分类问题时，模型的输出层的神经元个数与我们需要区分的类别数量不同。这个时候，我们就需要修改模型。\n修改模型所用到的函数或方法主要有以下几个：\n# 1. 增加：add_module 方法，可以增加单层也可以增加 Sequential vgg16.add_module(\u0026#34;mod\u0026#34;, nn.Sequential(OrderedDict([ # 可以不给每层取名 (\u0026#34;linear\u0026#34;, nn.Linear(1000, 256)), (\u0026#34;relu\u0026#34;, nn.ReLU()), (\u0026#34;softmax\u0026#34;, nn.Softmax(10)) ]))) vgg16.add_module(nn.Linear(64, 10)) # 2. 修改：直接使用 [idx] 和 `.` print(vgg16) # 先print查看模型结构 vgg16.classifier[6] = nn.Linear(64, 10) vgg16.classifier[6] = nn.Sequential( nn.Linear(64, 10), nn.Linear(10, 5) ) # 3. 删除：使用 del 即可 del vgg16.classifier 训练模型 下面提供一个训练示例：\n# * 3. Create the model object and Setting hyperparameters # create model object model = Model() if torch.cuda.is_available(): model = model.cuda() # define the loss function loss_fn = nn.CrossEntropyLoss() if torch.cuda.is_available(): loss_fn = loss_fn.cuda() # define the optimizer learning_rate = 0.01 optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # optimizer = torch.optim.Adam(model.parameters()) # Setting total_train_step = 0 total_test_step = 0 epoch = 30 # set tensorboard writer = SummaryWriter(\u0026#34;./logs_model\u0026#34;) # * 4. Training the model for i in range(epoch): print(f\u0026#34;------ Epoch {i} begin. ------\u0026#34;) start_time = time.time() # train model.train() # for some certain layer(such as dropout), See PyTorch.org. for data in train_dataloader: imgs, targets = data # move to gpu if torch.cuda.is_available(): imgs, targets = imgs.cuda(), targets.cuda() output = model(imgs) loss = loss_fn(output, targets) # optimize optimizer.zero_grad() loss.backward() optimizer.step() total_train_step = total_train_step + 1 if total_train_step % 100 == 0: print(f\u0026#34;Train step: {total_train_step} / Loss: {loss}\u0026#34;) end_time = time.time() print(f\u0026#34;tic {end_time-start_time}\u0026#34;) writer.add_scalar(\u0026#34;train_loss(step)\u0026#34;, loss, total_train_step) # test model.eval() # for some certain layer total_test_loss = 0 total_accuracy = 0 with torch.no_grad(): for data in test_dataloader: imgs, targets = data # move to gpu if torch.cuda.is_available(): imgs, targets = imgs.cuda(), targets.cuda() output = model(imgs) # compute accuracy accuracy = (output.argmax(1) == targets).sum() total_accuracy += accuracy loss = loss_fn(output, targets) total_test_loss += loss # Show and log test info print(ColorText.info(f\u0026#34;\\nTotal loss in test set: {total_test_loss}\u0026#34;)) print( ColorText.info(f\u0026#34;Total accuracy in test set: {total_accuracy / test_set_size}\u0026#34;) ) writer.add_scalar(\u0026#34;test_loss(epoch)\u0026#34;, total_test_loss, total_test_step) writer.add_scalar( \u0026#34;test_accuracy(epoch)\u0026#34;, total_accuracy / test_set_size, total_test_step ) total_test_step += 1 # Save Model torch.save(model, f\u0026#34;model_{i}.pt\u0026#34;) print(ColorText.info(f\u0026#34;..::Model {i} Saved..\u0026#34;)) writer.close() 附录 Tensorboard 的使用 示例：\n首先在脚本内记录数据到指定路径\nfrom torch.urils.tensorboard import SummaryWriter # writer 对象 writer = SummaryWriter(\u0026#34;./logs\u0026#34;) # log_dir # 记录数据 writer.add_scalar(\u0026#34;loss curve\u0026#34;, y_value, x_value) # writer.close() 然后，命令行用 Tensorboard 打开日志路径\ntensorboard --log_dir ./logs ","date":"2024年3月22日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/blog/2024/03/pytorch/","series":[],"smallImg":"","tags":[{"title":"深度学习","url":"/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"title":"PyTorch","url":"/tags/pytorch/"}],"timestamp":1711112701,"title":"PyTorch"},{"authors":[],"categories":[],"content":"W.I.P 动态规划学习笔记.\n动态规划的演进 Dfs -\u0026gt; 记忆化搜索 -\u0026gt; 动态规划 (倒序递推/逆序递推 -\u0026gt; 空间优化)\n实现记忆化搜索，dfs 函数的参数应当尽量少，不影响边界的参数不要添加。 想要剪枝，一般多用参数\n动态规划 什么是动态规划 动态规划（英语：Dynamic programming，简称 DP），是一种在数学、管理科学、计算机科学、经济学和生物信息学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法。 动态规划常常适用于有重叠子问题和最优子结构性质的问题。\n动态规划的核心思想： 动态规划最核心的思想，就在于拆分子问题，记住过往，减少重复计算。\n示例： 下题\n// P1359 租用游艇 // https://www.luogu.com.cn/problem/P1359 #include \u0026lt;bits/stdc++.h\u0026gt; #define int long long using namespace std; const int N = 1000200; int timee[210][210]; // 直接从 i 到 j 的时间 int n; int mintime[210][210]; // 子问题分解，记录从 i 到 j 的最短时间 signed main(void) { cin \u0026gt;\u0026gt; n; for (int i=1; i\u0026lt;=n; i++) { for (int j=i+1; j\u0026lt;=n; j++) { cin \u0026gt;\u0026gt; timee[i][j]; } } // input part for (int i=1; i\u0026lt;=n; i++) { // 递推，从 1 到 i 的最短时间 -\u0026gt; 从 1 到 n 的最短时间 for (int j=i; j\u0026gt;=1; j--) { // 最后一步 if (j == i) mintime[1][i] = timee[1][i]; //！注意 else mintime[1][i] = min(mintime[1][i], timee[j][i] + mintime[1][j]); } } cout \u0026lt;\u0026lt; mintime[1][n]; return 0; } ","date":"2024年3月22日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/blog/2024/03/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","series":[],"smallImg":"","tags":[{"title":"算法，动态规划","url":"/tags/%E7%AE%97%E6%B3%95%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"}],"timestamp":1711112641,"title":"动态规划"},{"authors":[],"categories":[],"content":"Hi there, I\u0026rsquo;m SymmFz.\nWelcome to my blog.\n","date":"2024年3月22日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/about/","series":[],"smallImg":"","tags":[],"timestamp":1711065600,"title":"关于我"},{"authors":[],"categories":[],"content":"","date":"1年1月1日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/offline/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"Offline"},{"authors":[],"categories":[],"content":"","date":"1年1月1日","img":"","lang":"zh-hans","langName":"简体中文","largeImg":"","permalink":"/contact/","series":[],"smallImg":"","tags":[],"timestamp":-62135596800,"title":"联系我们"}]
